{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T05:03:29.800596Z",
     "start_time": "2025-06-28T05:03:27.626942Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import AI_model\n",
    "import utils\n",
    "\n",
    "class Data_process:\n",
    "    def __init__(self, data_type):\n",
    "        self.data_type = data_type  # \"RML2016.10a\" or \"RML2016.10b\"\n",
    "        if data_type == 'RML2016.10a':\n",
    "            self.file_path = f'D:/Research/Dataset_nocode/{self.data_type}/RML2016.10a_dict.pkl'\n",
    "        elif data_type == 'RML2016.10b':\n",
    "            self.file_path = f'D:/Research/Dataset_nocode/{self.data_type}/RML2016.10b.dat'\n",
    "\n",
    "        self.global_comm_round = 20\n",
    "        self.num_locals = 3\n",
    "\n",
    "\n",
    "    def data_import(self):\n",
    "        with open(self.file_path, 'rb') as file:\n",
    "            pickle_data = pickle.load(file, encoding='latin1')\n",
    "\n",
    "        data_item = list(pickle_data.items())\n",
    "        data, SNR, label = [], [], []\n",
    "\n",
    "        for i in range(len(data_item)):\n",
    "            data.append(data_item[i][1])\n",
    "            for j in range(len(data_item[i][1])):\n",
    "                label.append(data_item[i][0][0])\n",
    "                SNR.append(data_item[i][0][1])\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_labels = label_encoder.fit_transform(label)\n",
    "        print(f\"RML Dataset Length - 1st (data): {data[0].shape}, 2nd element(SNR): {len(SNR)}, 3rd element(label): {integer_labels}\")\n",
    "\n",
    "        label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "        print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "        return data, SNR, integer_labels\n",
    "\n",
    "\n",
    "    def data_process(self, data, SNR, integer_labels, test_ratio=0.2):\n",
    "\n",
    "        def one_hot_to_label(one_hot_encoded):\n",
    "            lst = []\n",
    "            for i in range(len(one_hot_encoded)):\n",
    "                lst.append(np.argmax(one_hot_encoded[i]))\n",
    "\n",
    "            return lst\n",
    "\n",
    "        def one_hot_encode(labels):\n",
    "            labels_reshaped = labels.reshape(-1, 1)\n",
    "            encoder = OneHotEncoder(sparse_output=False)\n",
    "            one_hot_encoded = encoder.fit_transform(labels_reshaped)\n",
    "\n",
    "            return one_hot_encoded\n",
    "\n",
    "        OH_label = one_hot_encode(integer_labels)\n",
    "\n",
    "        if self.data_type == \"RML2016.10a\":\n",
    "            X_data = np.array(data).reshape(1000*len(data), 2, 128, 1)\n",
    "        elif self.data_type == \"RML2016.10b\":\n",
    "            X_data = np.array(data).reshape(6000*len(data), 2, 128, 1)\n",
    "        else:\n",
    "            print(\"data_type either RML2016.10a or RML2016.10b\")\n",
    "            return None\n",
    "\n",
    "        combined_data = list(zip(X_data, OH_label, SNR))\n",
    "        random.shuffle(combined_data)\n",
    "        shuffled_x_data, shuffled_y_label, shuffled_SNR = zip(*combined_data)\n",
    "        x, y, z = np.array(shuffled_x_data), np.array(shuffled_y_label), np.array(shuffled_SNR)\n",
    "\n",
    "        shuffled_indices = np.random.permutation(len(x))\n",
    "\n",
    "        #test_ratio = 0.2  ##########  0.2 Test / 0.8 Train\n",
    "        split_index = int(len(x) * (1 - test_ratio))\n",
    "\n",
    "        x_train, x_test = x[shuffled_indices[:split_index]], x[shuffled_indices[split_index:]]\n",
    "        y_train, y_test = y[shuffled_indices[:split_index]], y[shuffled_indices[split_index:]]\n",
    "        z_train, z_test = z[shuffled_indices[:split_index]], z[shuffled_indices[split_index:]]\n",
    "\n",
    "        print(f\"x_train shape: {x_train.shape}, y_train: {y.shape}, x_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "\n",
    "class MetaLearner:\n",
    "    def __init__(self, build_model_fn, custom_loss_fn, N_way, input_shape, meta_iters=100, meta_step_size=0.3, alpha=0.5):\n",
    "        self.build_model_fn = build_model_fn\n",
    "        self.custom_loss_fn = custom_loss_fn\n",
    "        self.meta_iters = meta_iters\n",
    "        self.meta_step_size = meta_step_size\n",
    "        self.num_classes = N_way\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.model = AI_model.build_model(self.num_classes)\n",
    "        self.model.build(input_shape=(None, *self.input_shape))\n",
    "        #self.model.compile(loss=lambda prototypes: self.custom_loss_fn(prototypes, self.alpha), metrics=['accuracy'])\n",
    "        #self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "    def build_model(self):\n",
    "        return self.build_model_fn(self.num_classes, self.input_shape)\n",
    "\n",
    "    def calculate_prototypes(self, embeddings, labels):\n",
    "        prototypes = []\n",
    "        for class_idx in range(self.num_classes):\n",
    "            class_embeddings = embeddings[labels == class_idx]\n",
    "            prototype = np.mean(class_embeddings, axis=0)\n",
    "            prototypes.append(prototype)\n",
    "        return np.array(prototypes)\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, get_data_fn, N_way, K_shot):\n",
    "        accuracies = []\n",
    "\n",
    "        for meta_iter in range(self.meta_iters):\n",
    "            frac_done = meta_iter / self.meta_iters\n",
    "            cur_step_size = (1 - frac_done) * self.meta_step_size\n",
    "            old_weights = self.model.get_weights()\n",
    "            train_data, test_data, proto_X, proto_y = get_data_fn(X_train, y_train, X_test, y_test, N_way=N_way, K_shot=K_shot, split=True)\n",
    "\n",
    "            # Step 1: Get prototypes\n",
    "            proto_embeddings = self.model.predict(proto_X, verbose=0)\n",
    "            proto_labels = tf.argmax(proto_y, axis=1)\n",
    "            prototypes = self.calculate_prototypes(proto_embeddings, proto_labels)\n",
    "\n",
    "            # Step 2: Set custom loss using current prototypes\n",
    "            self.model.compile(\n",
    "                loss=make_custom_loss(prototypes, alpha=self.alpha),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            #self.model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "            if train_data is None or test_data is None or len(train_data[0]) == 0 or len(test_data[0]) == 0:\n",
    "                print(f\"⚠️ Skipping iteration {meta_iter+1} due to empty train/test data.\")\n",
    "                continue\n",
    "\n",
    "            # Step 3: Train on one episode\n",
    "            result = self.model.fit(train_data[0], train_data[1], epochs=100, validation_data=(test_data[0], test_data[1]), verbose=0)\n",
    "            val_acc = np.max(result.history['val_accuracy'])\n",
    "            accuracies.append(val_acc)\n",
    "\n",
    "            print(f\"Meta Iter {meta_iter+1}/{self.meta_iters} - Val Acc: {val_acc:.4f}, Max: {np.max(accuracies):.4f}\")\n",
    "\n",
    "            # Step 4: Meta-update\n",
    "            new_weights = self.model.get_weights()\n",
    "            updated_weights = [\n",
    "                old + (new - old) * cur_step_size\n",
    "                for old, new in zip(old_weights, new_weights)\n",
    "            ]\n",
    "            self.model.set_weights(updated_weights)\n",
    "\n",
    "        return accuracies"
   ],
   "id": "b29823b6ed76dfab",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-28T05:03:29.808360Z",
     "start_time": "2025-06-28T05:03:29.801969Z"
    }
   },
   "source": [
    "def cosine_distance(a, b):\n",
    "    a = tf.math.l2_normalize(a, axis=-1)\n",
    "    b = tf.math.l2_normalize(b, axis=-1)\n",
    "    return 1 - tf.reduce_sum(a * b, axis=-1)\n",
    "\n",
    "def mahalanobis_distance(x, y, cov_inv):\n",
    "    diff = x - y\n",
    "    left = tf.matmul(diff, cov_inv)\n",
    "    dist = tf.reduce_sum(left * diff, axis=-1)\n",
    "    return dist\n",
    "\n",
    "'''def calculate_prototypes(embeddings, labels, num_classes):\n",
    "    \"\"\"Calculate mean embedding (prototype) for each class.\"\"\"\n",
    "    prototypes = []\n",
    "    for class_idx in range(num_classes):\n",
    "        class_embeddings = embeddings[labels == class_idx]\n",
    "        prototype = tf.reduce_mean(class_embeddings, axis=0)\n",
    "        prototypes.append(prototype)\n",
    "    return tf.stack(prototypes)'''\n",
    "\n",
    "def prototype_loss(embedding, labels, prototypes):\n",
    "    \"\"\"Compute distance between embedding and true class prototype.\"\"\"\n",
    "    distances = tf.norm(embedding[:, None, :] - prototypes[None, :, :], axis=-1)  #L2 dist.\n",
    "    #distances = tf.reduce_sum(tf.abs(embedding[:, None, :] - prototypes[None, :, :]), axis=-1)  #L1 dist.\n",
    "    #distances = cosine_distance(embedding[:, None, :], prototypes[None, :, :]) #cosine sim.\n",
    "    #prototype = tf.math.l2_normalize(tf.reduce_mean(class_embeddings, axis=0), axis=0) => add if dist. is cosine sim.\n",
    "\n",
    "    class_indices = tf.cast(tf.argmax(labels, axis=-1), tf.int32)\n",
    "    true_distances = tf.gather_nd(distances, tf.stack([tf.range(tf.shape(distances)[0]), class_indices], axis=1))\n",
    "    return tf.reduce_mean(true_distances)\n",
    "\n",
    "\n",
    "def make_custom_loss(prototypes, alpha):\n",
    "    \"\"\"\n",
    "    Return a custom loss function combining cross-entropy and prototype loss.\n",
    "    This is needed because prototype is fixed per task/episode.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        class_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        proto_loss = prototype_loss(y_pred, y_true, prototypes)\n",
    "        return alpha * class_loss + (1 - alpha) * proto_loss\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def get_data(X_train, y_train, X_test, y_test, N_way, K_shot, split=True):\n",
    "    \"\"\"\n",
    "    Prepares a K-shot N-way classification task.\n",
    "    Assumes y_train and y_test are one-hot.\n",
    "    \"\"\"\n",
    "    y_train = np.argmax(y_train, axis=1)\n",
    "    y_test = np.argmax(y_test, axis=1)\n",
    "\n",
    "    unique_classes = np.unique(y_train)\n",
    "    selected_classes = np.random.choice(unique_classes, size=N_way, replace=False)\n",
    "\n",
    "    X_support, y_support = [], []\n",
    "    X_query, y_query = [], []\n",
    "\n",
    "    for cls in selected_classes:\n",
    "        idxs = np.where(y_train == cls)[0]\n",
    "        np.random.shuffle(idxs)\n",
    "        if len(idxs) < K_shot * 2:\n",
    "            print(\"HERE\")\n",
    "            continue\n",
    "\n",
    "        support_idxs = idxs[:K_shot]\n",
    "        query_idxs = idxs[K_shot:K_shot*2]  # equal size query set\n",
    "\n",
    "        X_support.append(X_train[support_idxs])\n",
    "        y_support.append(y_train[support_idxs])\n",
    "\n",
    "        if split:\n",
    "            X_query.append(X_train[query_idxs])\n",
    "            y_query.append(y_train[query_idxs])\n",
    "\n",
    "    X_support = np.concatenate(X_support, axis=0)\n",
    "    y_support = np.concatenate(y_support, axis=0)\n",
    "\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(selected_classes)\n",
    "    y_support_1hot = lb.transform(y_support)\n",
    "\n",
    "    if split:\n",
    "        X_query = np.concatenate(X_query, axis=0)\n",
    "        y_query = np.concatenate(y_query, axis=0)\n",
    "        y_query_1hot = lb.transform(y_query)\n",
    "\n",
    "        # Also provide support set again for calculating prototypes\n",
    "        return (X_support, y_support_1hot), (X_query, y_query_1hot), X_support, y_support_1hot\n",
    "    else:\n",
    "        return X_support, y_support_1hot\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T05:03:31.900331Z",
     "start_time": "2025-06-28T05:03:29.881341Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data, SNR, labels = Data_process(\"RML2016.10a\").data_import()\n",
    "X_train, y_train, X_test, y_test = Data_process(\"RML2016.10a\").data_process(data, SNR, labels)"
   ],
   "id": "f95972dacbab6cf2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RML Dataset Length - 1st (data): (1000, 2, 128), 2nd element(SNR): 220000, 3rd element(label): [9 9 9 ... 3 3 3]\n",
      "Label Mapping: {np.str_('8PSK'): np.int64(0), np.str_('AM-DSB'): np.int64(1), np.str_('AM-SSB'): np.int64(2), np.str_('BPSK'): np.int64(3), np.str_('CPFSK'): np.int64(4), np.str_('GFSK'): np.int64(5), np.str_('PAM4'): np.int64(6), np.str_('QAM16'): np.int64(7), np.str_('QAM64'): np.int64(8), np.str_('QPSK'): np.int64(9), np.str_('WBFM'): np.int64(10)}\n",
      "x_train shape: (176000, 2, 128, 1), y_train: (220000, 11), x_test: (44000, 2, 128, 1), y_test: (44000, 11)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-06-28T05:03:31.909378Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#num_classes = y_test.shape[-1]\n",
    "k_shot_sample = 10\n",
    "N_way_class = 5\n",
    "\n",
    "meta = MetaLearner(\n",
    "    build_model_fn=AI_model.build_model,                  # Your model builder function\n",
    "    custom_loss_fn=make_custom_loss,             # Prototype-aware loss generator\n",
    "    N_way=N_way_class,                     # E.g. 2\n",
    "    input_shape=X_train.shape[1:],               # E.g. (63,) or (3, 21, 1)\n",
    "    meta_iters=100,                              # Number of meta-training loops\n",
    "    meta_step_size=0.001,                          # Reptile meta step\n",
    "    alpha=0.7                                     # Balancing classification vs prototype loss\n",
    ")\n",
    "\n",
    "\n",
    "accuracies = meta.train(\n",
    "    X_train, y_train, X_test, y_test,\n",
    "    get_data_fn=get_data,\n",
    "    N_way=N_way_class, K_shot=k_shot_sample\n",
    ")"
   ],
   "id": "ee44e4cea19256eb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meta Iter 1/100 - Val Acc: 0.4400, Max: 0.4400\n",
      "Meta Iter 2/100 - Val Acc: 0.3600, Max: 0.4400\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021DFBBED6C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function TensorFlowTrainer.make_predict_function.<locals>.one_step_on_data_distributed at 0x0000021DFBBED6C0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "Meta Iter 3/100 - Val Acc: 0.3600, Max: 0.4400\n",
      "Meta Iter 4/100 - Val Acc: 0.3400, Max: 0.4400\n",
      "Meta Iter 5/100 - Val Acc: 0.3200, Max: 0.4400\n",
      "Meta Iter 6/100 - Val Acc: 0.4200, Max: 0.4400\n",
      "Meta Iter 7/100 - Val Acc: 0.3800, Max: 0.4400\n",
      "Meta Iter 8/100 - Val Acc: 0.4000, Max: 0.4400\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "plt.plot(accuracies)",
   "id": "58b23cbec1056b11",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "accuracies",
   "id": "7c7e1302afc56819",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "id": "45f4d3035826b72a",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
