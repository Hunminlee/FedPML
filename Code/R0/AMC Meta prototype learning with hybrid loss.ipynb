{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import model\n",
    "\n",
    "\n",
    "class MetaLearner:\n",
    "    def __init__(self, build_model_fn, custom_loss_fn, num_classes, input_shape, meta_iters=100, meta_step_size=0.3, alpha=0.5):\n",
    "        self.build_model_fn = build_model_fn\n",
    "        self.custom_loss_fn = custom_loss_fn\n",
    "        self.meta_iters = meta_iters\n",
    "        self.meta_step_size = meta_step_size\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.alpha = alpha\n",
    "\n",
    "        self.model = model.build_model(self.num_classes)\n",
    "        self.model.compile(loss=lambda y_true, y_pred: self.custom_loss_fn(y_true, y_pred, self.alpha),\n",
    "                           metrics=['accuracy'])\n",
    "\n",
    "    def build_model(self):\n",
    "        return self.build_model_fn(self.num_classes, self.input_shape)\n",
    "\n",
    "    def calculate_prototypes(self, embeddings, labels):\n",
    "        prototypes = []\n",
    "        for class_idx in range(self.num_classes):\n",
    "            class_embeddings = embeddings[labels == class_idx]\n",
    "            prototype = np.mean(class_embeddings, axis=0)\n",
    "            prototypes.append(prototype)\n",
    "        return np.array(prototypes)\n",
    "\n",
    "\n",
    "    def train(self, X_train, y_train, X_test, y_test, get_data_fn, K_shot):\n",
    "        accuracies = []\n",
    "\n",
    "        for meta_iter in range(self.meta_iters):\n",
    "            frac_done = meta_iter / self.meta_iters\n",
    "            cur_step_size = (1 - frac_done) * self.meta_step_size\n",
    "            old_weights = self.model.get_weights()\n",
    "\n",
    "            train_data, test_data, proto_X, proto_y = get_data_fn(X_train, y_train, X_test, y_test, K_shot, split=True)\n",
    "\n",
    "            # Step 1: Get prototypes\n",
    "            proto_embeddings = self.model.predict(proto_X, verbose=0)\n",
    "            proto_labels = tf.argmax(proto_y, axis=1)\n",
    "            prototypes = calculate_prototypes(proto_embeddings, proto_labels, self.num_classes)\n",
    "\n",
    "            # Step 2: Set custom loss using current prototypes\n",
    "            self.model.compile(\n",
    "                loss=make_custom_loss(prototypes, alpha=self.alpha),\n",
    "                metrics=['accuracy']\n",
    "            )\n",
    "\n",
    "            # Step 3: Train on one episode\n",
    "            result = self.model.fit(train_data, epochs=100, validation_data=test_data, verbose=0)\n",
    "            val_acc = np.max(result.history['val_accuracy'])\n",
    "            accuracies.append(val_acc)\n",
    "\n",
    "            print(f\"Meta Iter {meta_iter+1}/{self.meta_iters} - Val Acc: {val_acc:.4f}, Max: {np.max(accuracies):.4f}\")\n",
    "\n",
    "            # Step 4: Meta-update\n",
    "            new_weights = self.model.get_weights()\n",
    "            updated_weights = [\n",
    "                old + (new - old) * cur_step_size\n",
    "                for old, new in zip(old_weights, new_weights)\n",
    "            ]\n",
    "            self.model.set_weights(updated_weights)\n",
    "\n",
    "        return accuracies\n"
   ],
   "id": "8947f26d6a746f5d"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def cosine_distance(a, b):\n",
    "    a = tf.math.l2_normalize(a, axis=-1)\n",
    "    b = tf.math.l2_normalize(b, axis=-1)\n",
    "    return 1 - tf.reduce_sum(a * b, axis=-1)\n",
    "\n",
    "def mahalanobis_distance(x, y, cov_inv):\n",
    "    diff = x - y\n",
    "    left = tf.matmul(diff, cov_inv)\n",
    "    dist = tf.reduce_sum(left * diff, axis=-1)\n",
    "    return dist\n",
    "\n",
    "\n",
    "\n",
    "def calculate_prototypes(embeddings, labels, num_classes):\n",
    "    \"\"\"Calculate mean embedding (prototype) for each class.\"\"\"\n",
    "    prototypes = []\n",
    "    for class_idx in range(num_classes):\n",
    "        class_embeddings = embeddings[labels == class_idx]\n",
    "        prototype = tf.reduce_mean(class_embeddings, axis=0)\n",
    "        prototypes.append(prototype)\n",
    "    return tf.stack(prototypes)\n",
    "\n",
    "def prototype_loss(embedding, labels, prototypes):\n",
    "    \"\"\"Compute distance between embedding and true class prototype.\"\"\"\n",
    "    distances = tf.norm(embedding[:, None, :] - prototypes[None, :, :], axis=-1)  #L2 dist.\n",
    "    #distances = tf.reduce_sum(tf.abs(embedding[:, None, :] - prototypes[None, :, :]), axis=-1)  #L1 dist.\n",
    "    #distances = cosine_distance(embedding[:, None, :], prototypes[None, :, :]) #cosine sim.\n",
    "    #prototype = tf.math.l2_normalize(tf.reduce_mean(class_embeddings, axis=0), axis=0) => add if dist. is cosine sim.\n",
    "\n",
    "    class_indices = tf.argmax(labels, axis=-1)\n",
    "    true_distances = tf.gather_nd(distances, tf.stack([tf.range(tf.shape(distances)[0]), class_indices], axis=1))\n",
    "    return tf.reduce_mean(true_distances)\n",
    "\n",
    "def make_custom_loss(prototypes, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Return a custom loss function combining cross-entropy and prototype loss.\n",
    "    This is needed because prototype is fixed per task/episode.\n",
    "    \"\"\"\n",
    "    def loss(y_true, y_pred):\n",
    "        class_loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)\n",
    "        proto_loss = prototype_loss(y_pred, y_true, prototypes)\n",
    "        return alpha * class_loss + (1 - alpha) * proto_loss\n",
    "    return loss\n"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "meta = MetaLearner(\n",
    "    build_model_fn=build_model,                  # Your model builder function\n",
    "    custom_loss_fn=make_custom_loss,             # Prototype-aware loss generator\n",
    "    num_classes=num_classes,                     # E.g. 2\n",
    "    input_shape=X_train.shape[1:],               # E.g. (63,) or (3, 21, 1)\n",
    "    meta_iters=100,                              # Number of meta-training loops\n",
    "    meta_step_size=0.3,                          # Reptile meta step\n",
    "    alpha=0.7                                     # Balancing classification vs prototype loss\n",
    ")"
   ],
   "id": "cb831a84c89e7ef0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "accuracies = meta.train(\n",
    "    X_train=X_train,             # full training set\n",
    "    y_train=y_train,             # one-hot encoded\n",
    "    X_test=X_test,               # meta-validation set\n",
    "    y_test=y_test,\n",
    "    get_data_fn=get_data,        # your K-shot episodic data function\n",
    "    K_shot=5                     # few-shot setting (e.g., 5-shot)\n",
    ")"
   ],
   "id": "f95972dacbab6cf2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(accuracies)\n",
    "plt.xlabel(\"Meta Iteration\")\n",
    "plt.ylabel(\"Validation Accuracy\")\n",
    "plt.title(\"Reptile Meta-Learning Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ],
   "id": "58b23cbec1056b11"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
