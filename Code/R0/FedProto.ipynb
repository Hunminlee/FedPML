{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "Fedproto 구현"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T05:28:47.553028Z",
     "start_time": "2025-06-28T05:28:44.999068Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from collections import deque\n",
    "import AI_model\n",
    "import pickle\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import random\n",
    "\n",
    "\n",
    "class Queue:\n",
    "    def __init__(self):\n",
    "        self.items = deque()\n",
    "\n",
    "    def enqueue(self, item):\n",
    "        self.items.append(item)\n",
    "\n",
    "    def dequeue(self):\n",
    "        if not self.is_empty():\n",
    "            return self.items.popleft()\n",
    "        else:\n",
    "            print(\"Queue is empty\")\n",
    "\n",
    "    def is_empty(self):\n",
    "        return len(self.items) == 0\n",
    "\n",
    "    def size(self):\n",
    "        return len(self.items)\n",
    "\n",
    "def build_model(num_classes):\n",
    "    model = AI_model.build_model(num_classes)\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['acc'])\n",
    "    return model\n",
    "\n",
    "\n",
    "def extract_local_prototypes(model, x_local, y_local, num_classes):\n",
    "    #dense_features, conv_features = model(x_local, training=False, feature_extraction=True)\n",
    "    dense_features = model(x_local, training=False, feature_extraction=True)\n",
    "\n",
    "    dense_features_flattened = tf.reshape(dense_features, [dense_features.shape[0], -1]).numpy()\n",
    "    #conv_features_flattened = tf.reshape(conv_features, [conv_features.shape[0], -1]).numpy()\n",
    "\n",
    "    labels = np.argmax(y_local, axis=1)\n",
    "\n",
    "    dense_prototypes = np.zeros((num_classes, dense_features_flattened.shape[1]))\n",
    "    #conv_prototypes = np.zeros((num_classes, conv_features_flattened.shape[1]))\n",
    "\n",
    "    for i in range(num_classes):\n",
    "        class_features_dense = dense_features_flattened[labels == i]\n",
    "        if len(class_features_dense) > 0:\n",
    "            dense_prototypes[i] = np.mean(class_features_dense, axis=0)\n",
    "\n",
    "        #class_features_conv = conv_features_flattened[labels == i]\n",
    "        #if len(class_features_conv) > 0:\n",
    "        #    conv_prototypes[i] = np.mean(class_features_conv, axis=0)\n",
    "\n",
    "    return dense_prototypes#, conv_prototypes\n",
    "\n",
    "\n",
    "def update_global_prototypes(local_prototypes_list, num_classes):\n",
    "    global_prototypes = np.zeros((num_classes, local_prototypes_list[0].shape[1]))\n",
    "    for i in range(num_classes):\n",
    "        class_prototypes = np.array([proto[i] for proto in local_prototypes_list])\n",
    "        global_prototypes[i] = np.mean(class_prototypes, axis=0)\n",
    "    return global_prototypes\n",
    "\n",
    "def classify_with_global_prototypes(model, x, global_prototypes):\n",
    "    features = model(x, training=False, feature_extraction=True).numpy()\n",
    "    distances = np.sqrt(((features[:, np.newaxis, :] - global_prototypes[np.newaxis, :, :]) ** 2).sum(axis=2))\n",
    "    predictions = np.argmin(distances, axis=1)\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def prototype_loss(y_true, y_pred, features, global_prototypes):\n",
    "    global_prototypes = tf.convert_to_tensor(global_prototypes, dtype=tf.float32)\n",
    "    c = tf.argmax(y_true, axis=1)\n",
    "    prototypes = tf.gather(global_prototypes, c)\n",
    "    features_flattened = tf.reshape(features, [features.shape[0], -1])\n",
    "    distance = tf.reduce_mean(tf.square(features_flattened - prototypes))\n",
    "    return distance\n",
    "\n",
    "\n",
    "def train_step(model, x_batch, y_batch, optimizer, global_prototypes, epoch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        features = model(x_batch, training=True, feature_extraction=True)\n",
    "        y_pred = model(x_batch, training=True)\n",
    "        loss = tf.keras.losses.categorical_crossentropy(y_batch, y_pred)\n",
    "        prototype_loss_value = prototype_loss(y_batch, y_pred, features, global_prototypes)\n",
    "        total_loss = loss + prototype_loss_value\n",
    "\n",
    "    gradients = tape.gradient(total_loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return total_loss, prototype_loss_value\n",
    "\n",
    "\n",
    "def train_model(model, x_train, y_train, global_prototypes, batch_size, epochs):\n",
    "    Total_loss, P_loss = [], []\n",
    "    optimizer = tf.keras.optimizers.Adam()\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(batch_size)\n",
    "    for epoch in range(epochs):\n",
    "        for x_batch, y_batch in dataset:\n",
    "            total_loss, prototype_loss_value = train_step(model, x_batch, y_batch, optimizer, global_prototypes, epochs)\n",
    "            Total_loss.append(total_loss.numpy())\n",
    "            P_loss.append(prototype_loss_value.numpy())\n",
    "\n",
    "    return Total_loss, P_loss"
   ],
   "id": "200a4878577f5156",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-28T05:30:49.971746Z",
     "start_time": "2025-06-28T05:30:49.963180Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class Get_Data:\n",
    "\n",
    "    def __init__(self, data_type):\n",
    "        self.data_type = data_type  # \"RML2016.10a\" or \"RML2016.10b\"\n",
    "        if data_type == 'RML2016.10a':\n",
    "            self.file_path = f'D:/Research/Dataset_nocode/{self.data_type}/RML2016.10a_dict.pkl'\n",
    "        elif data_type == 'RML2016.10b':\n",
    "            self.file_path = f'D:/Research/Dataset_nocode/{self.data_type}/RML2016.10b.dat'\n",
    "\n",
    "        self.global_comm_round = 20\n",
    "        self.num_locals = 3\n",
    "\n",
    "\n",
    "    def data_import(self):\n",
    "        with open(self.file_path, 'rb') as file:\n",
    "            pickle_data = pickle.load(file, encoding='latin1')\n",
    "\n",
    "        data_item = list(pickle_data.items())\n",
    "        data, SNR, label = [], [], []\n",
    "\n",
    "        for i in range(len(data_item)):\n",
    "            data.append(data_item[i][1])\n",
    "            for j in range(len(data_item[i][1])):\n",
    "                label.append(data_item[i][0][0])\n",
    "                SNR.append(data_item[i][0][1])\n",
    "\n",
    "        label_encoder = LabelEncoder()\n",
    "        integer_labels = label_encoder.fit_transform(label)\n",
    "        print(f\"RML Dataset Length - 1st (data): {data[0].shape}, 2nd element(SNR): {len(SNR)}, 3rd element(label): {integer_labels}\")\n",
    "\n",
    "        label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))\n",
    "        print(\"Label Mapping:\", label_mapping)\n",
    "\n",
    "        return data, SNR, integer_labels\n",
    "\n",
    "\n",
    "    def data_process(self, data, SNR, integer_labels, test_ratio=0.2):\n",
    "\n",
    "        def one_hot_to_label(one_hot_encoded):\n",
    "            lst = []\n",
    "            for i in range(len(one_hot_encoded)):\n",
    "                lst.append(np.argmax(one_hot_encoded[i]))\n",
    "\n",
    "            return lst\n",
    "\n",
    "        def one_hot_encode(labels):\n",
    "            labels_reshaped = labels.reshape(-1, 1)\n",
    "            encoder = OneHotEncoder(sparse_output=False)\n",
    "            one_hot_encoded = encoder.fit_transform(labels_reshaped)\n",
    "\n",
    "            return one_hot_encoded\n",
    "\n",
    "        OH_label = one_hot_encode(integer_labels)\n",
    "\n",
    "        if self.data_type == \"RML2016.10a\":\n",
    "            X_data = np.array(data).reshape(1000*len(data), 2, 128, 1)\n",
    "        elif self.data_type == \"RML2016.10b\":\n",
    "            X_data = np.array(data).reshape(6000*len(data), 2, 128, 1)\n",
    "        else:\n",
    "            print(\"data_type either RML2016.10a or RML2016.10b\")\n",
    "            return None\n",
    "\n",
    "        combined_data = list(zip(X_data, OH_label, SNR))\n",
    "        random.shuffle(combined_data)\n",
    "        shuffled_x_data, shuffled_y_label, shuffled_SNR = zip(*combined_data)\n",
    "        x, y, z = np.array(shuffled_x_data), np.array(shuffled_y_label), np.array(shuffled_SNR)\n",
    "\n",
    "        shuffled_indices = np.random.permutation(len(x))\n",
    "\n",
    "        #test_ratio = 0.2  ##########  0.2 Test / 0.8 Train\n",
    "        split_index = int(len(x) * (1 - test_ratio))\n",
    "\n",
    "        x_train, x_test = x[shuffled_indices[:split_index]], x[shuffled_indices[split_index:]]\n",
    "        y_train, y_test = y[shuffled_indices[:split_index]], y[shuffled_indices[split_index:]]\n",
    "        z_train, z_test = z[shuffled_indices[:split_index]], z[shuffled_indices[split_index:]]\n",
    "\n",
    "        print(f\"x_train shape: {x_train.shape}, y_train: {y.shape}, x_test: {x_test.shape}, y_test: {y_test.shape}\")\n",
    "\n",
    "        return x_train, y_train, x_test, y_test\n",
    "\n",
    "    def main(self):\n",
    "        data, SNR, integer_labels = self.data_import()\n",
    "        X_train, y_train, X_test, y_test = self.data_process(data, SNR, integer_labels, test_ratio=0.2)\n",
    "\n",
    "        return X_train, y_train, X_test, y_test"
   ],
   "id": "e85fcca5ccab9cc1",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "g_round, num_local = 5, 5\n",
    "FedEMG_loss, FedEMG_acc = [], []\n",
    "num_classes = 11\n",
    "local_clients = 3\n",
    "local_models = [build_model(num_classes) for _ in range(local_clients)]\n",
    "\n",
    "for idx in range(local_clients): #Subject\n",
    "    globals()['Q_X_{}'.format(idx)], globals()['Q_Y_{}'.format(idx)] = Queue(), Queue()\n",
    "\n",
    "for epoch in range(g_round):   #Repetition\n",
    "    for idx in range(local_clients): #Subject\n",
    "        Current_rep = epoch+1\n",
    "        _, _, x_test_local, y_test_local = Get_Data(\"RML2016.10a\").main()\n",
    "\n",
    "        if idx==0 and epoch==0:\n",
    "            client_data_indices = np.random.choice(len(x_test_local), size=500, replace=False)\n",
    "            X_test, Y_test = x_test_local[client_data_indices], y_test_local[client_data_indices]\n",
    "        else:\n",
    "            client_data_indices = np.random.choice(len(x_test_local), size=500, replace=False)\n",
    "            X_test = np.concatenate((X_test, x_test_local[client_data_indices]))\n",
    "            Y_test = np.concatenate((Y_test, y_test_local[client_data_indices]))\n",
    "\n",
    "\n",
    "for epoch in range(g_round):   #Repetition\n",
    "    print(f'\\nGlobal Epoch {epoch+1}/{g_round} start\\n\\n')\n",
    "    local_prototypes = []\n",
    "\n",
    "    for idx in range(local_clients): #Subject\n",
    "        Current_rep = epoch+1\n",
    "        X_train, Y_train, x_test_local, y_test_local = Get_Data(\"RML2016.10a\").main()\n",
    "\n",
    "\n",
    "        if epoch > 3:\n",
    "            globals()['Q_X_{}'.format(idx)].dequeue()\n",
    "            globals()['Q_Y_{}'.format(idx)].dequeue()\n",
    "\n",
    "        if epoch > 0:\n",
    "            for i in range(globals()['Q_X_{}'.format(idx)].size()):\n",
    "                X_train = np.concatenate((X_train, globals()['Q_X_{}'.format(idx)].items[i]), axis=0)\n",
    "                Y_train = np.concatenate((Y_train, globals()['Q_Y_{}'.format(idx)].items[i]), axis=0)\n",
    "\n",
    "\n",
    "\n",
    "        result = local_models[idx].fit(X_train, Y_train, epochs=200, batch_size=256, verbose=0)\n",
    "\n",
    "        #print(f'{idx} intra-subject performance ====> 1')\n",
    "        #local_models[idx].evaluate(x_test_local, y_test_local, verbose=1)\n",
    "        #print(f'{idx} inter-subject performance =======> 1')\n",
    "        #local_models[idx].evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "        random_indices = np.random.choice(len(X_train), size=1000, replace=False)\n",
    "        X_for_Q, Y_for_Q = X_train[random_indices], Y_train[random_indices]\n",
    "\n",
    "        globals()['Q_X_{}'.format(idx)].enqueue(X_for_Q)\n",
    "        globals()['Q_Y_{}'.format(idx)].enqueue(Y_for_Q)\n",
    "        prototype = extract_local_prototypes(local_models[idx], X_train, Y_train, num_classes)\n",
    "        local_prototypes.append(prototype)\n",
    "\n",
    "\n",
    "    global_prototypes = update_global_prototypes(local_prototypes, num_classes)\n",
    "    for idx in range(local_clients): #Subject\n",
    "        Current_rep = epoch+1\n",
    "\n",
    "        X_train, Y_train, x_test_local, y_test_local = Get_Data(\"RML2016.10a\").main()\n",
    "\n",
    "        batch_size = 256\n",
    "\n",
    "        train_model(local_models[idx], X_train, Y_train, global_prototypes, batch_size, epochs=30)  # 글로벌 프로토타입을 사용하여 모델 학습\n",
    "        print(f'{idx} intra-subject performance ====> 2')\n",
    "        local_models[idx].evaluate(x_test_local, y_test_local, verbose=1)\n",
    "        print(f'{idx} inter-subject performance =======> 2')\n",
    "        local_models[idx].evaluate(X_test, Y_test, verbose=1)\n",
    "\n",
    "    for idx in range(local_clients): #Subject\n",
    "        Current_rep = epoch+1\n",
    "\n",
    "        X_train, Y_train, x_test_local, y_test_local = Get_Data(\"RML2016.10a\").main()\n",
    "\n",
    "        if epoch > 0:\n",
    "            for i in range(globals()['Q_X_{}'.format(idx)].size()):\n",
    "                X_train = np.concatenate((X_train, globals()['Q_X_{}'.format(idx)].items[i]), axis=0)\n",
    "                Y_train = np.concatenate((Y_train, globals()['Q_Y_{}'.format(idx)].items[i]), axis=0)\n",
    "\n",
    "        result = local_models[idx].fit(X_train, Y_train, epochs=200, batch_size=256, verbose=0)\n",
    "\n",
    "        print(f'{idx} intra-subject performance ====> 3')\n",
    "        local_models[idx].evaluate(x_test_local, y_test_local, verbose=1)\n",
    "        print(f'{idx} inter-subject performance =======> 3')\n",
    "        local_models[idx].evaluate(X_test, Y_test, verbose=1)\n",
    "\n"
   ],
   "id": "b2cd9d0f384f92f5"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
